{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI for Medicine Course 3 Week 1 lecture notebook - Model Training/Tuning Basics with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this exercise! You're going to be exploring the `sklearn` library, including an overview of its underlying data types and methods for tweaking a model's hyperparameters. You'll be using the same data from the previous lecture notebook. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "First import all the packages that you need for this assignment. \n",
    "\n",
    "\n",
    "- `pandas` is what you'll use to manipulate your data\n",
    "- `numpy`  is a library for mathematical and scientific operations\n",
    "- `sklearn` has many efficient tools for machine learning and statistical modeling\n",
    "- `itertools` helps with hyperparameter (grid) searching\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "Run the next cell to import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Set the random seed for consistent output\n",
    "np.random.seed(18)\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv(\"dummy_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for training: 37\n",
      "Number of observations for testing: 13\n"
     ]
    }
   ],
   "source": [
    "# Import module to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the label\n",
    "y = data.outcome\n",
    "\n",
    "# Get the features\n",
    "X = data.drop('outcome', axis=1) \n",
    "\n",
    "# Get training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "print(f\"Number of observations for training: {y_train.size}\")\n",
    "print(f\"Number of observations for testing: {y_test.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a logistic regression to the training data. `Sklearn` allows you to provide arguments that override the defaults. \n",
    "\n",
    "The default solver is `lbfgs`.  \n",
    "- Lbfgs stands for ['Limited Memory BFGS'](https://en.wikipedia.org/wiki/Limited-memory_BFGS), and is an efficient and popular method for fitting models.\n",
    "- The solver is set explicitly here for learning purposes; if you do not set the solver parameter explicitly, the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function will use its default solver, which is lbfgs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it fits the training data, `sklearn` also prints out the model's hyperparameters.  \n",
    "- Here, these are the default hyperparameters for `sklearn's` logistic regression classifier.\n",
    "- Another way to check these parameters is the `get_params()` method of the classifier.\n",
    "\n",
    "You should spend some time checking out the [documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) to get a deeper understanding of what's going on. One important thing to note is that each classifier has different hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "To predict with the classifier, use the `predict()` method. \n",
    "- This returns a `numpy` array containing the predicted class for each observation in the test set, as you can see by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions is of type: <class 'numpy.ndarray'>\n",
      "predictions has shape: (13,)\n",
      "predicted class for 10th element in test set: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to predict labels from the features of the test set\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# View the prediction type, shape, and print out a sample prediction\n",
    "print(f\"predictions is of type: {type(predictions)}\")\n",
    "print(f\"predictions has shape: {predictions.shape}\")\n",
    "print(f\"predicted class for 10th element in test set: {predictions[9]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction probability\n",
    "\n",
    "When a model predicts that a label is 1 rather than 0, it may help you to know if the model was predicting 1 with a 51% probability or 90% probability; in other words, how confident is that prediction?\n",
    "\n",
    "You can get the model's probability of predicting each of the class. \n",
    "- To do this, use the `predict_proba()` method. \n",
    "- The resulting array will have a shape that matches the number of classes for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_probs is of type: <class 'numpy.ndarray'>\n",
      "prediction_probs has shape: (13, 2)\n",
      "probabilities for first element in test set: [0.42348297 0.57651703]\n"
     ]
    }
   ],
   "source": [
    "prediction_probs = lr.predict_proba(X_test)\n",
    "print(f\"prediction_probs is of type: {type(prediction_probs)}\")\n",
    "print(f\"prediction_probs has shape: {prediction_probs.shape}\")\n",
    "print(f\"probabilities for first element in test set: {prediction_probs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 patients in the test set.  Each patient's label could be either 0 or 1, so the prediction probability has 13 rows and 2 columns.  To know which column refers to lable 0 and which refers to label 1, you can check the `.classes_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the order of the `classes_` array is 0, then 1, column 0 of the prediction probabilities has label 0, and column 1 has label 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print these for the first 5 elements of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element number: 0\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.42348296737845204\n",
      "Probability of predicting class 1: 0.576517032621548\n",
      "\n",
      "Element number: 1\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.4914968703166631\n",
      "Probability of predicting class 1: 0.5085031296833369\n",
      "\n",
      "Element number: 2\n",
      "Predicted class: 1\n",
      "Probability of predicting class 0: 0.483088763245346\n",
      "Probability of predicting class 1: 0.516911236754654\n",
      "\n",
      "Element number: 3\n",
      "Predicted class: 0\n",
      "Probability of predicting class 0: 0.86953653498578\n",
      "Probability of predicting class 1: 0.13046346501422001\n",
      "\n",
      "Element number: 4\n",
      "Predicted class: 0\n",
      "Probability of predicting class 0: 0.8470774295731573\n",
      "Probability of predicting class 1: 0.15292257042684265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Element number: {i}\")\n",
    "    print(f\"Predicted class: {predictions[i]}\")\n",
    "    print(f\"Probability of predicting class 0: {prediction_probs[i][0]}\")\n",
    "    print(f\"Probability of predicting class 1: {prediction_probs[i][1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the predicted class matches the class with a higher probability of being predicted. Since you're dealing with `numpy` arrays, you can simply slice them and get specific information, such as the probability of predicting class 1 for all elements in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57651703, 0.50850313, 0.51691124, 0.13046347, 0.15292257,\n",
       "       0.26162479, 0.50831618, 0.3190805 , 0.37250246, 0.47736442,\n",
       "       0.15743244, 0.51193665, 0.26832495])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve prediction probabilities for label 1, for all patients\n",
    "prediction_probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the predictive power of a classifier can be increased if a good set of hyperparameters is defined. This is known as model tuning. \n",
    "\n",
    "For this process, you'll need a classifier, an appropriate evaluation metric, and a set of parameters to test. Since this is a dummy example, you'll use the default metric for the logistic regression classifier: the **mean accuracy**.\n",
    "\n",
    "### Mean Accuracy\n",
    "Mean Accuracy is the number of correct predictions divided by total predictions. This can be computed with the `score()` method. \n",
    "\n",
    "Let's begin by checking the performance of your out-of-the-box logit classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to tweak this model's default parameters. You can pass a dictionary containing the values you specify to the classifier when you instantiate it. Notice that these must be passed as keyword arguments, or `kwargs`, which are created by using the ** prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweaked hyperparameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': False, 'intercept_scaling': 1, 'max_iter': 500, 'multi_class': 'ovr', 'n_jobs': 1, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Mean Accuracy: 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "# Choose hyperparameters and place them as key-value pairs in a dictionary\n",
    "params = {\n",
    "    'solver': 'liblinear',\n",
    "    'fit_intercept': False,\n",
    "    'penalty': 'l1',\n",
    "    'max_iter': 500\n",
    "}\n",
    "\n",
    "# Pass in the dictionary as keyword arguments to the model\n",
    "lr_tweaked = LogisticRegression(**params)\n",
    "\n",
    "# Train the model\n",
    "lr_tweaked.fit(X_train, y_train)\n",
    "\n",
    "# View hyper-parameters\n",
    "print(f\"Tweaked hyperparameters: {lr_tweaked.get_params()}\\n\")\n",
    "\n",
    "# Evaluate the model with the mean accuracy\n",
    "print(f\"Mean Accuracy: {lr_tweaked.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the tweaked parameters is worse than the original! However, there might still be some combination of parameters that increase the predictive power of your logit classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different hyperparameters\n",
    "Testing this can be daunting considering all the possible parameter combinations. Let's try something \n",
    "\n",
    "To get started, you'll apply `itertools.product()` to create all the combinations of parameters. \n",
    "- Notice that the iterable (in this case a list of the lists of parameters) must be passed as *args to the `product()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters and place in a dictionary\n",
    "hyperparams = {\n",
    "    'solver': [\"liblinear\"],\n",
    "    'fit_intercept': [True, False],\n",
    "    'penalty': [\"l1\", \"l2\"],\n",
    "    'class_weight': [None, \"balanced\"]\n",
    "}\n",
    "# Get the values of hyperparams and convert them to a list of lists\n",
    "hp_values = list(hyperparams.values())\n",
    "hp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get every combination of the hyperparameters\n",
    "for hp in itertools.product(*hp_values):\n",
    "    print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the combinations of hyperparams\n",
    "for hp in itertools.product(*hp_values):\n",
    "\n",
    "    # Create the model with the hyperparams\n",
    "    estimator = LogisticRegression(solver=hp[0],\n",
    "                                   fit_intercept=hp[1],\n",
    "                                   penalty=hp[2],\n",
    "                                   class_weight=hp[3])\n",
    "    # Fit the model\n",
    "    estimator.fit(X_train, y_train)\n",
    "    print(f\"Parameters used: {hp}\")\n",
    "    print(f\"Mean accuracy of the model: {estimator.score(X_test, y_test)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the graded assignment, you will take a more generalizable approach that doesn't require you to explicitly specify each hyperparameter.\n",
    "\n",
    "That is, instead of:\n",
    "\n",
    "```CPP\n",
    "LogisticRegression(solver=hp[0],fit_intercept=hp[1],...\n",
    "```\n",
    "\n",
    "You'll be able to write:\n",
    "```CPP\n",
    "LogisticRegression(**params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like none of these models beats the original! This won't always be the case, so next time the opportunity arises, you'll be able to check this for yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "This is essentially grid search.  You'll be implementing customized grid search in the graded assignment.  \n",
    "- Note that even though sci-kit learn provides a grid search function, it uses K-fold cross validation, which you won't want to do in the assignment, which is why you will implement grid search yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on completing this lecture notebook! \n",
    "\n",
    "By now, you should feel more comfortable with the `sklearn` library and how it works. You also created a grid search from scratch by leveraging the `itertools` library. Nice work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
