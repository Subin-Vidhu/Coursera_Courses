{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning\n","\n","Estimated time needed: **45** minutes\n","\n","Most of the real-world data, that the data scientist work with, are raw data, meaning that it can contain repeated, missing, and irrelevant entries of information. Hence, if this data is used in any machine learning analysis, it will result in low accuracy or incorrect prediction. For this reason, data cleaning, also known as data cleansing, is an important technique that comes prior to any model building.\n","\n","In this notebook, we will take a look at some of the common data cleaning techniques that data scientists may use to prepare their data for analysis.\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Use Log function to transform the data\n","*   Handle the duplicates\n","*   Handle the missing values\n","*   Standardize and normalize the data\n","*   Handle the outliers\n"]},{"cell_type":"markdown","metadata":{},"source":["***\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Setup**\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for mathematical operations.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for visualizing the data.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for statistical computations.\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Import the required libraries**\n"]},{"cell_type":"markdown","metadata":{},"source":["The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings \n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","\n","import seaborn as sns \n","import matplotlib.pylab as plt\n","%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from scipy.stats import norm\n","from scipy import stats"]},{"cell_type":"markdown","metadata":{},"source":["## **Reading and understanding our data**\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the Ames_Housing_Data.tsv file, hosted on IBM Cloud object storage. The Ames housing dataset examines features of houses sold in Ames (a small city in the state of Iowa in the United States) during the 2006â€“2010 timeframe.\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/Ames_Housing_Data1.tsv\", sep='\\t')\n","housing.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["We can find more information about the features and types using the `info()`  method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.info()"]},{"cell_type":"markdown","metadata":{},"source":["According to the output above, we have 2930 entries, 0 to 2929, as well as 81 features. The \"Non-Null Count\" column shows the number of non-null entries.  If the count is 2930 then there is no missing values for that particular feature. 'SalePrice' is our target or response variable and the rest of the features are our predictor variables.\n","\n","We also have a mix of numerical (28 int64 and 11 float64) and object data types.\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's use the `describe()` function to show the count, mean, min, max of the sale price attribute.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing[\"SalePrice\"].describe()"]},{"cell_type":"markdown","metadata":{},"source":["From the above analysis, it is important to note that the minimum value is greater than 0. Also, there is a big difference between the minimum value and the 25th percentile. It is bigger than the 75th percentile and the maximum value. This means that our data might not be normally distributed (an important assumption for linear regression analysis), so will check for normality in the Log Transform section.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 1\n"]},{"cell_type":"markdown","metadata":{},"source":["The `describe()` function reveals the statistical information about the numeric attributes. To reveal some information about our categorical (object) attributes, we can use `value_counts()` function. In this exercise, describe all categories of the 'Sale Condition' attribute.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n"," housing[\"Sale Condition\"].value_counts()\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Looking for Correlations**\n"]},{"cell_type":"markdown","metadata":{},"source":["Before proceeding with the data cleaning, it is useful to establish a correlation between the response variable (in our case the sale price) and other predictor variables, as some of them might not have any major impact in determining the price of the house and will not be used in the analysis.  There are many ways to discover correlation between the target variable and the rest of the features. Building pair plots, scatter plots, heat maps, and a correlation matrixes are the most common ones. Below, we will use the `corr()` function to list the top features based on the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) (measures how closely two sequences of numbers are correlated). Correlation coefficient can only be calculated on the numerical attributes (floats and integers), therefore, only the numeric attributes will be selected.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hous_num = housing.select_dtypes(include = ['float64', 'int64'])\n","hous_num_corr = hous_num.corr()['SalePrice'][:-1] # -1 means that the latest row is SalePrice\n","top_features = hous_num_corr[abs(hous_num_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n","print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(top_features), top_features))"]},{"cell_type":"markdown","metadata":{},"source":["Above, there are 11 features, with coefficients greater than 0.5, that are strongly correlated with the sale price.\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's generate some par plots to visually inspect the correlation between some of these features and the target variable. We will use seaborns `sns.pairplot()` function for this analysis. Also, building pair plots is one of the possible ways to spot the outliers that might be present in the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(0, len(hous_num.columns), 5):\n","    sns.pairplot(data=hous_num,\n","                x_vars=hous_num.columns[i:i+5],\n","                y_vars=['SalePrice'])"]},{"cell_type":"markdown","metadata":{},"source":["From Pearsons Correlation Coefficients and pair plots, we can draw some conclusions about the features that are most strongly correlated to the 'SalePrice'. They are: 'Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Garage Area', and others.\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Log Transformation**\n"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we are going to inspect whether our 'SalePrice' data are normally distributed. The assumption of the normal distribution must be met in order to perform any type of regression analysis. There are several ways to check for this assumption, however here, we will use the visual method, by plotting the 'SalePrice' distribution using the `distplot()` function from the `seaborn` library.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sp_untransformed = sns.distplot(housing['SalePrice'])"]},{"cell_type":"markdown","metadata":{},"source":["As the plot shows, our 'SalePrice' deviates from the normal distribution. It has a longer tail to the right, so we call it a positive skew. In statistics *skewness* is a measure of asymmetry of the distribution. In addition to skewness, there is also a kurtosis, parameter which refers to the pointedness of a peak in the distribution curve. Both skewness and kurtosis are frequently used together to characterize the distribution of data.\n"]},{"cell_type":"markdown","metadata":{},"source":["Here, we can simply use the `skew()` function to calculate our skewness level of the `SalePrice`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Skewness: %f\" % housing['SalePrice'].skew())"]},{"cell_type":"markdown","metadata":{},"source":["The range of skewness for a fairly symmetrical bell curve distribution is between -0.5 and 0.5; moderate skewness is -0.5 to -1.0 and 0.5 to 1.0; and highly skewed distribution is < -1.0 and > 1.0. In our case, we have \\~1.7, so it is considered  highly skewed data.\n","\n","Now, we can try to transform our data, so it looks more normally distributed. We can use the `np.log()` function from the `numpy` library to perform log transform. This [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) contains more information about the numpy log transform.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log_transformed = np.log(housing['SalePrice'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sp_transformed = sns.distplot(log_transformed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Skewness: %f\" % (log_transformed).skew())"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the log method transformed the 'SalePrice' distribution into a more symmetrical bell curve and the skewness level now is -0.01, well within the range.\n","\n","There are other ways to correct for skewness of the data. For example, Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library). To learn more about these two methods, please check out this [article](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 2\n"]},{"cell_type":"markdown","metadata":{},"source":["In this exercise, visually inspect the 'Lot Area' feature. If there is any skewness present, apply log transform to make it more normally distributed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","la_plot = sns.distplot(housing['Lot Area'])\n","print(\"Skewness: %f\" % housing['Lot Area'].skew())\n","la_log = np.log(housing['Lot Area'])\n","print(\"Skewness: %f\" % la_log.skew())\n","\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Handling the Duplicates**\n"]},{"cell_type":"markdown","metadata":{},"source":["As mentioned in the video, having duplicate values can effect our analysis, so it is good to check whether there are any duplicates in our data. We will use pandas `duplicated()` function and search by the 'PID' column, which contains a unique index number for each entry.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["duplicate = housing[housing.duplicated(['PID'])]\n","duplicate"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, there is one duplicate row in this dataset. To remove it, we can use pandas `drop_duplicates()` function. By default, it removes all duplicate rows based on all the columns.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dup_removed = housing.drop_duplicates()\n","dup_removed "]},{"cell_type":"markdown","metadata":{},"source":["An alternative way to check if there are any duplicated Indexes in our dataset is using `index.is_unique` function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.index.is_unique"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 3\n"]},{"cell_type":"markdown","metadata":{},"source":["In this exercise try to remove duplicates on a specific column by setting the subset equal to the column that contains the duplicate, such as 'Order'.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","removed_sub = housing.drop_duplicates(subset=['Order'])\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Handling the Missing Values**\n"]},{"cell_type":"markdown","metadata":{},"source":["### Finding the Missing Values\n"]},{"cell_type":"markdown","metadata":{},"source":["For easier detection of missing values, pandas provides the `isna()`, `isnull()`, and `notna()` functions. For more information on pandas missing values please check out this [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"]},{"cell_type":"markdown","metadata":{},"source":["To summarize all the missing values in our dataset, we will use `isnull()` function. Then, we will add them all up, by using `sum()` function, sort them with `sort_values()` function, and plot the first 20 columns (as the majority of our missing values fall within first 20 columns), using the `bar plot` function from the `matplotlib` library.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total = housing.isnull().sum().sort_values(ascending=False)\n","total_select = total.head(20)\n","total_select.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\n","\n","plt.xlabel(\"Columns\", fontsize = 20)\n","plt.ylabel(\"Count\", fontsize = 20)\n","plt.title(\"Total Missing Values\", fontsize = 20)"]},{"cell_type":"markdown","metadata":{},"source":["There are several options for dealing with missing values. We will use 'Lot Frontage' feature to analyze for missing values.\n"]},{"cell_type":"markdown","metadata":{},"source":["1.  We can drop the missing values, using `dropna()` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.dropna(subset=[\"Lot Frontage\"])"]},{"cell_type":"markdown","metadata":{},"source":["Using this method, all the rows, containing null values in 'Lot Frontage' feature, for example, will be dropped.\n"]},{"cell_type":"markdown","metadata":{},"source":["2.  We can drop the whole attribute (column), that contains missing values, using the `drop()` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.drop(\"Lot Frontage\", axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Using this method, the entire column containing the null values will be dropped.\n"]},{"cell_type":"markdown","metadata":{},"source":["3.  We can replace the missing values (zero, the mean, the median, etc.), using `fillna()` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["median = housing[\"Lot Frontage\"].median()\n","median"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing[\"Lot Frontage\"].fillna(median, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.tail()"]},{"cell_type":"markdown","metadata":{},"source":["Index# 2927, containing a missing value in the \"Lot Frontage\", now has been replaced with the median value.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 4\n"]},{"cell_type":"markdown","metadata":{},"source":["In this exercise, let's look at 'Mas Vnr Area' feature and replace the missing values with the mean value of that column.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","mean = housing[\"Mas Vnr Area\"].mean()\n","housing[\"Mas Vnr Area\"].fillna(mean, inplace = True)   \n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Feature Scaling**\n"]},{"cell_type":"markdown","metadata":{},"source":["One of the most important transformations we need to apply to our data is feature scaling.  There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n","\n","Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.\n","\n","Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance.\n"]},{"cell_type":"markdown","metadata":{},"source":["Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs. For more information on `scikit-learn` [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) and [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) please visit their respective documentation websites.\n"]},{"cell_type":"markdown","metadata":{},"source":["First, we will normalize our data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["norm_data = MinMaxScaler().fit_transform(hous_num)\n","norm_data"]},{"cell_type":"markdown","metadata":{},"source":["Note the data is now a `ndarray`\n"]},{"cell_type":"markdown","metadata":{},"source":["we can also standardize our data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaled_data = StandardScaler().fit_transform(hous_num)\n","scaled_data"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 5\n"]},{"cell_type":"markdown","metadata":{},"source":["In this exercise, use `StandardScaler()` and `fit_transform()` functions to standardize the 'SalePrice' feature only.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","scaled_sprice = StandardScaler().fit_transform(housing['SalePrice'][:,np.newaxis]) \n","scaled_sprice\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Handling the Outliers**\n"]},{"cell_type":"markdown","metadata":{},"source":["### Finding the Outliers\n"]},{"cell_type":"markdown","metadata":{},"source":["In statistics, an outlier is an observation point that is distant from other observations. An outlier can be due to some mistakes in data collection or recording, or due to natural high variability of data points. How to treat an outlier highly depends on our data or the type of analysis to be performed. Outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n","\n","There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Uni-variate Analysis\n"]},{"cell_type":"markdown","metadata":{},"source":["A box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. To learn more about box plots please click [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"]},{"cell_type":"markdown","metadata":{},"source":["Here, we will use a box plot for the 'Lot Area' and the 'SalePrice' features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.boxplot(x=housing['Lot Area'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.boxplot(x=housing['SalePrice'])"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from these two plots, we have some points that are plotted outside the box plot area and that greatly deviate from the rest of the population. Whether to remove or keep them will greatly depend on the understanding of our data and the type of analysis to be performed. In this case, the points that are outside of our box plots in the 'Lot Area' and the 'Sale Price' might be the actual true data points and do not need to be removed.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Bi-variate Analysis\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we will look at the bi-variate analysis of the two features, the sale price, 'SalePrice', and the ground living area, 'GrLivArea', and plot the scatter plot of the relationship between these two parameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["price_area = housing.plot.scatter(x='Gr Liv Area',\n","                      y='SalePrice')"]},{"cell_type":"markdown","metadata":{},"source":["From the above graph, there are two values above 5000 sq. ft. living area that deviate from the rest of the population and do not seem to follow the trend. It can be speculated why this is happening but for the purpose of this lab we can delete them.\n","\n","The other two observations on the top are also deviating from the rest of the points but they also seem to be following the trend, so, perhaps, they can be kept.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Deleting the Outliers\n"]},{"cell_type":"markdown","metadata":{},"source":["First, we will sort all of our 'Gr Liv Area' values and select only the last two.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.sort_values(by = 'Gr Liv Area', ascending = False)[:2]"]},{"cell_type":"markdown","metadata":{},"source":["Now we will use the pandas `drop()` function to remove these two rows.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outliers_dropped = housing.drop(housing.index[[1499,2181]])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_plot = outliers_dropped.plot.scatter(x='Gr Liv Area',\n","                                         y='SalePrice')"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, we do not have the last two points of the 'Gr Liv Area' anymore.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 6\n"]},{"cell_type":"markdown","metadata":{},"source":["In this exercise, determine whether there are any outliers in the 'Lot Area' feature. You can either plot the box plot for the 'Lot Area', perform a bi-variate analysis by making a scatter plot between the 'SalePrice' and the 'Lot Area', or use the Z-score analysis. If there re any outliers, remove them from the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Enter your code and run the cell"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","sns.boxplot(x=housing['Lot Area'])\n","price_lot = housing.plot.scatter(x='Lot Area', y='SalePrice')   \n","housing['Lot_Area_Stats'] = stats.zscore(housing['Lot Area'])\n","housing[['Lot Area','Lot_Area_Stats']].describe().round(3)\n","housing.sort_values(by = 'Lot Area', ascending = False)[:1]\n","lot_area_rem = housing.drop(housing.index[[957]])\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary><strong>Answer</strong> (Click Here)</summary>\n","    &emsp; &emsp; <code>\n","There seems to be one outlier, the very last point in the 'Lot Area' is too far from the rest of the group. Also, according to the Z-score, the standard deviation of that point exceeds the threshhold of 3.\n","</code>\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Z-score Analysis\n"]},{"cell_type":"markdown","metadata":{},"source":["Z-score is another way to identify outliers mathematically. Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. In another words, Z-score is the value that quantifies relationship between a data point and a standard deviation and mean values of a group of points. Data points which are too far from zero will be treated as the outliers. In most of the cases, a threshold of 3 or -3 is used. For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.\n","\n","To learn more about Z-score, please visit this [Wikipedia](https://en.wikipedia.org/wiki/Standard_score?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) site.\n"]},{"cell_type":"markdown","metadata":{},"source":["Below, we are using Z-score function from `scipy` library to detect the outliers in our 'Low Qual Fin SF' parameter. To learn more about `scipy.stats`, please visit this [link](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing['LQFSF_Stats'] = stats.zscore(housing['Low Qual Fin SF'])\n","housing[['Low Qual Fin SF','LQFSF_Stats']].describe().round(3)"]},{"cell_type":"markdown","metadata":{},"source":["The scaled results show a mean of 0.000 and a standard deviation of 1.000, indicating that the transformed values fit the z-scale model. The max value of 22.882 is further proof of the presence of outliers, as it falls well above the z-score limit of +3.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Congratulations! - You have completed the lab\n"]},{"cell_type":"markdown","metadata":{},"source":["## Author\n"]},{"cell_type":"markdown","metadata":{},"source":["[Svitlana Kramar](https://www.linkedin.com/in/svitlana-kramar?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By | Change Description                   |\n","| ----------------- | ------- | ---------- | ------------------------------------ |\n","| 2021-11-30        | 0.1     | Svitlana   | Added the Log Transformation section |\n","| 2022-01-18        | 0.2     | Svitlana   | Added the Introduction               |\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}